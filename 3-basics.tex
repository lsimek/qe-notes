\section{Basics of probability}

% uniform integrability
% borel-cantelli
% inequalities
%
%
\subsection{Inequalities}
\subsubsection{Jensen}
Suppose \( X \) is a random variable and \( \varphi \colon \R \to \R \) is
a \emph{convex} function. Then
\begin{equation}
	\varphi(\E X) \le \E (\varphi X).
\end{equation}

For \( c = \E X \), consider an affine function \( h(x) = ax + b \) lying under \( \varphi \) (\( h \le \varphi \))
but with \( h(c) = \varphi(c) \) (the graph of \( h \) is separating two disjoint convex sets: epigraph of \( \varphi  \) and \( \oo{c,\infty} \times \oo{-\infty, c} \)). Then
\begin{equation}
	\varphi(\E X) = h(\E X) = a\E X + b = \E (aX + b) = \E h(X) \le \E \varphi(X).
\end{equation}


\subsubsection{\holder}
For any measurable functions \( f,g \) and real \( p,q,r > 0 \) such that \( 1/r = 1/p + 1/q \),
\begin{equation}
	\norm{fg}_r \le \norm f_p \norm g_q.
\end{equation}

Clearly we can take \( r=1, \norm f_p = 1, \norm g_q = 1 \) (\emph{unless} \( f=0 \) or \( g=0 \) which is a trivial edge-case). It is true that
\begin{equation}\label{eq:holderlemma}
	uv \le \frac{u^p}p + \frac{v^q}q, \quad u,v\ge 0.
\end{equation}

By setting \( u=\abs{f(x)} \) and \( v=\abs{g(x)} \) and integrating, we get
\begin{equation}
	\int \abs{fg} \le p^{-1} \int \abs f^p + q^{-1} \int \abs g^q = p^{-1} + q^{-1} = 1.
\end{equation}

\asteriskline
\emph{Note} that \( 1/p+1/q=1 \) can be written as
\begin{equation}\label{eq:conjuequiv}
	(p-1)(q-1) = 1
\end{equation}
or in the more general case \( (p/r-1)(q/r-1)=1 \) or \( (p-r)(q-r)=r^2 \).

\asteriskline
The inequality~\eqref{eq:holderlemma} can be proven via simple calculus or from
(see~\cite[p.\ 26]{kalle}):
\begin{equation}
	uv \le \int _0^u x^{p-1} \D x + \int_0^v y^{q-1} \D y = \frac {u^p}p + \frac {v^q}q.
\end{equation}
This is because \( uv \) is the area of the rectangle with sides \(  u \) and \( v \)
on \( x \)- and \( y \)- axes respectively, \( \int_0^u x^{p-1} \D x \) is the area
under the graph of \( x^{p-1} \) up to \( u \) and
\( \int_0^v y^{q-1} \D y \) is the area to the left of said graph up to \( v \).
These regions always cover the rectangle with equality when \( x^{p-1} \) passes
through \( (u,v) \), that is \( v = u^{p-1} \).


\subsubsection{Minkowski}
For \( p > 1 \) (the case \( p=\infty \) is easy),
\begin{equation}
	\norm {f+g}_p \le \norm f_p + \norm g_p.
\end{equation}

Firstly,
\begin{equation}\label{eq:prmink1}
	\int \abs{f+g}^p = \int \abs{f+g} \abs{f+g}^{p-1} \le
	\int \abs f \abs{f+g}^{p-1} + \int \abs g \abs{f+g}^{p-1}.
\end{equation}
The integral \( \int \abs f \abs {f+g}^{p-1} \) can be interpreted as
the \( 1 \)-norm of a product, so applying \holder \ we get
\begin{equation}
	\int \abs f \abs{f+g}^{p-1} \le \norm f_p \ob { \norm{f+g}_p^p }^{1/q}.
\end{equation}
With the analogous inequality for the other part of~\eqref{eq:prmink1}
we get
\begin{equation}
	\int \abs{f+g}^p \le \norm{f+g}_p^{p/q} \ug{\norm f_p + \norm g_p}.
\end{equation}
Dividing we get the required result (\( p-p/q =1\)).

\subsubsection{Markov/Chebishev-type inequalities}
The variant is from~\cite[p.\ 29]{du}. For a function \( \varphi \colon \R \to \R_{\ge 0} \),
\( A \) measurable and denoting \( m_A = \inf \vit{\varphi(y) \st y \in A} \),
\begin{equation}
	m_A \P(X \in A) \le \E(\varphi(X) \whi X \in A) \le \E \varphi(X).
\end{equation}
The \emph{proof} is from taking expectations in
\begin{equation}
	m_A 1_A(X) \le \varphi(X) 1_A(X) \le \varphi(X).
\end{equation}

\asteriskline

For \( X \ge 0  \), \( A = \co{a, \infty} \) for some \( a \ge 0 \) and \( \varphi \equiv 1 \) we
get Markov's inequality
\begin{equation}
	\P(X \ge a ) \le \frac{\E X} a.
\end{equation}
If \( X \) is not nonnegative, we need to take \( \varphi(x) = \abs x \).

For \( A = \co{a, \infty} \) for some \( a \ge 0 \) and \( \varphi(x) = (x-\E X)^2 \)
we get Chebishev's inequality
\begin{equation}
	\P( \abs{X - \E X} \ge a  ) \le \frac {\var X}{a^2}.
\end{equation}
The variant with \( \varphi(x) = x^2 \) is also legitimate:
\begin{equation}
	\P(\abs X \ge a) \le \frac{\E X^2}{a^2}.
\end{equation}

\subsubsection{Tails of the normal distribution}
The following is direct (Nourdin, p.\ 39):
\begin{align}
	\ob{\frac 1x - \frac 1{x^3}} e^{-x^2/2}
	 & = \int_x^\infty e^{-y^2/2} \ob{1- \frac 3{y^4}} \D y \\
	 & \le \int_x^\infty e^{-y^2/2} \D y \le
	\frac 1x \int_x^\infty ye^{-y^2/2} \D y = \frac 1x e^{-x^2/2}.
\end{align}

\subsection{Borel--Cantelli lemma}
For events \( A_j \),
\begin{equation}
	\sum_{j \ge 1} \P(A_j) < \infty \implies \P(A_j \io) = 0.
\end{equation}

\emph{Note} that
\begin{align}
	\left\{ A_j \io \right\}  & = \left\{ \sum_{j \ge 1} 1_{A_j} = \infty \right\},   \\
	\left\{ A_j \ult \right\} & = \left\{ \sum_{j \ge 1} 1_{A_j^c} < \infty \right\}.
\end{align}

If \( \sum_j \P(A_j) < \infty \) then
\begin{equation}
	\sum_{j \ge 1} P(A_j) = \sum_{j \ge 1} \E \ob{1_{A_j}} = \E \ob{\sum_{j \ge 1} 1_{A_j}}
\end{equation}
is \( < \infty \), so in particular \( \P\ob{\sum_j 1_{A_j} = \infty }= 0.\)


\asteriskline
The \emph{converse} holds when \( A_j \) are \emph{independent}.
Note
\begin{equation}
	\vit{A_j \io}^c = \ob{\bigcap_{n \ge 1} \bigcup_{k \ge n} A_k}^c
	= \bigcup_{n \ge 1} \bigcap_{k \ge n} A_k^c.
\end{equation}
Then
\begin{equation}
	\begin{aligned}
		1 & = \P(\text{not } A_j \io)                            \\
		  & = \lim_{n \ub} \P\ob{\bigcap_{k \ge n} A_k^c}
		= \lim_{n \ub} \prod_{k \ge n} \ug{1-\P(A_k)}            \\
		  & \le \lim_{n \ub} \exp \ob{- \sum_{k \ge n} \P(A_k)},
	\end{aligned}
\end{equation}
therefore \( \lim_{n \ub} \sum_{k \ge n} \P(A_k) = 0 \) which is equivalent
to \( \sum_{n \ge 1} \P(A_n) < \infty \).

\emph{Note} also that we used \( 1-x \le e^{-x}  \) for all \( x \).

\subsection{Convergence of random variables}
\subsubsection{Characterizing weak convergence I (expectation version)}
It holds that \( X_n \konvd X \), meaning
\( F_n(x) \rightarrow F(x) \) for all \( x \) such that \( F \) is continuous at \( x \),
\emph{if and only if} \( \E g(X_n) \to \E g(X) \) for all
\emph{continuous and bounded} functions \( g \).

First \emph{assume weak convergence}. There exist variables \( Y_n, Y \) with distributions
\( F_n, F \) such that \( Y_n \konvas Y \) (see below). Then also \( g(Y_n) \konvas g(Y) \) and
\begin{equation}
	\E g(X_n) = \E g(Y_n) \rightarrow \E g(Y) = \E g(X)
\end{equation}
using the \emph{bounded convergence} theorem.

Now the \emph{converse}. We want \( \E g(X_n) \to \E g(X) \)
where \( g = 1_{ \oc{\infty, x}  } \), but these functions are not continuous. The idea
then is to approximate them by continuous \( g \)'s.

Fix an \( x \in \R \) and for all \( r > 0 \) define
\begin{itemize}
	\item \( g_{r+} \) as \( 1 \) up to \( x \), \( 0 \) from \( x+1/r \) onwards and linear inbetween;
	\item \( g_{r-} \) as \( 1 \) up to \( x - 1/r \), \( 0 \) from \( x \) onwards and linear inbetween.
\end{itemize}
Then we have
\begin{equation}
	\limsup_{n \ub} \P(X_n \le x) \le \lim_{n \ub} \E g_{r+}(X_n) = \E g_{r+}(X) \to \P(X \le x).
\end{equation}
The lattermost convergence holds for all \( x \) due to
\begin{equation}
	\P(X \le x) \le \E g_{r+} (X) \le P(X \le x + 1/r)
\end{equation}
and the right-continuity of \( F \).

The \( \liminf \) side is obtained using \( g_{r-} \):
\begin{equation}
	\liminf_{n \ub} \P(X_n \le x) \ge \lim_{n \ub} \E g_{r-}(X_n) = \E g_{r-}(X) \to \P(X \le x),
\end{equation}
with the lattermost convergence being true for \( x \not \in \discont F \), since only then does
\begin{equation}
	\P(X \le x - 1/r) \le \E g_{r-}(X) \le \P(X \le x)
\end{equation}
yield the desired sandwiching.

\asteriskline
Earlier we used the following \emph{lemma}: if \( F_n \konvw F \) there exist
variables with those distributions such that \( Y_n \konvas Y_n \).
The common probability space is defined as usual
\begin{equation}
	\Omega = \cc{0,1}, \quad \mathcal F = \text{Borel sets}, \quad \P = \text{Lebesgue measure on} \cc{0,1},
\end{equation}
and the variables by
\begin{equation}
	Y_n(\omega) = \sup \vit{x \in \R \st F_n(x) < \omega}, \quad \omega \in \cc{0,1}.
\end{equation}

For proof that \( Y_n \konvas Y \) see~\cite[p.\ 118]{du}.

\subsubsection{Characterizing weak convergence II (portmanteau theorem)}


\subsubsection{Characterizing convergence in probability}

\subsubsection{Continuous mapping theorems}
Assume \( X_n \konvas X \) with \( g \) a measurable function such that
\( \P(X \in \discont g) = 0 \). \emph{Then} \( g(X_n) \konvas g(X) \).
This simply follows from
\begin{equation}
	\vit{ g(X_n) \to g(X)} \nd \vit{X_n \to X} \cap \vit{X \not \in \discont g}
\end{equation}

\asteriskline

Assume \( X_n \konvd X \) with \( g \) a measurable function such that
\( \P(X \in \discont g) = 0 \). \emph{Then} \( g(X_n) \konvd g(X) \). In particular and
if \( g \) is bounded, \( \E g(X_n) \to \E g(X) \).

Again let \( Y_n \konvas Y \). For any bounded continous \( f \) is
\( f \circ g \) also bounded and \( \discont(f \circ g) \pd \discont g \).
Thus \( f(g(Y_n)) \konvas f(g(Y)) \) and \( \E f(g(Y_n)) \to \E f(g(Y)) \) follows
by the bounded convergence theorem.

\asteriskline

Assume \( X_n \konvp X \) with \( g \) a measurable function such that
\( \P(X \in \discont g) = 0 \). \emph{Then} \( g(X_n) \konvp g(X) \).

See~\cite[p.\ 103]{kalle}.
Fix a subsequence \( N' \pd \N \).
The convergence \( X_n \konvp X \)
implies the convergence \( X_n \konvas X \) along some \( N'' \pd N' \).
Then also \( g(X_n) \konvas g(X) \) along \( N'' \) and
\( g(X_n) \konvas g(X) \) due to the uniqueness of the a.s.-limit. Since \( N' \) was arbitrary,
it follows that \( g(X_n) \konvp g(X) \).

\subsubsection{a.s. implies \( \P \)}
For any \( \varepsilon > 0 \),
\begin{align}
	\vit{X_n \not \to X} & \nd \vit{ \abs{X_n - X} \ge \varepsilon \text{ i.o.}}                   \\
	                     & \implies \P \ob{\abs{X_n-X} \ge \varepsilon \text{ i.o.}} = 0           \\
	                     & \implies \sum_{n = 1}^\infty \P(\abs{X_n - X} \ge \varepsilon) < \infty \\
	                     & \implies \lim_{n \ub} \P(\abs{X_n-X} \ge \varepsilon) = 0.
\end{align}

\subsubsection{\( \L^p \) implies \( \P \)}
We use a Markov-type inequality
\begin{equation}
	\P(\abs{X_n - X} \ge \varepsilon) \le \frac{\E \abs{X_n-X}^p}{\varepsilon^p} \un.
\end{equation}
\asteriskline

The \emph{converse} will hold if \( X_n-X \) is bounded by some \( M > 0 \), which is specifically true
if both \( X \) and all \( X_n \) are bounded. Then
\begin{equation}
	\E \abs{X_n-X}^p = \E \ob{\abs {X_n-X}^p \wedge M^p } \le
	\varepsilon \P(\abs{X_n-X} < \varepsilon) +
	M^p \P(\abs{X_n-X} \ge \varepsilon)
\end{equation}
which goes to \( 0 \) as \( \varepsilon \downarrow 0 \).

\subsubsection{\( \P \) implies d}
\subsubsection{Cauchyness in probability and in \( \L^p \)}


\subsection{Uniform integrability}
\subsubsection{Definition and characterization}
See~\cite[p.\ 106]{kalle}. A family \( (X_j)_{j \in J} \) of random variables is uniformly integrable if
\begin{equation}
	\lim_{r\ub} \sup_{j \in J} \E \ob{\abs{X_j} \whi \abs {X_j} > r} = 0.
\end{equation}
If it's a sequence \( (X_n)_{n \in \N} \) of \emph{integrable} variables, then
\begin{equation}
	\lim_{r\ub} \limsup_{n \ub} \E \ob{\abs{X_n} \whi \abs {X_n} > r} = 0
\end{equation}
\emph{suffices}, because
\begin{equation}
	\lim_{r \ub} \ug{ \limsup_{n \ub} \E \ob{\abs{X_n} \whi \abs{X_n} > r} - \sup_{n \in \N} \E \ob{\abs{X_n} \whi \abs{X_n} > r} } = 0,
\end{equation}
since both parts themselves tend to \( 0 \).

\asteriskline
For uniform integrability, boundedness in \( \L^p \) for some \( p > 1 \) (meaning \( \sup_j \E \abs{X_j}^p < \infty \)) \emph{suffices}.
We use the Markov-type inequality: \( \P(\abs{X_j} \ge x) \le x^{-p}\E\abs{X_j}^p  \) to get
\begin{align}
	\E (\abs{X_j} \whi \abs{X_j} > r ) & = \int_0^\infty \P(\abs{X_j} \cdot 1_{\vit{\abs{X_j} > r}} \ge x) \D x
	= \int_r^\infty \P(\abs{X_j} \ge x) \D x                                                                    \\ &\le \E \abs{X_j}^p \int_r^\infty x^{-p} \D x
	= \frac 1{p-1}r^{1-p} \E \abs{X_j} ^p.
\end{align}
for all \( j \in J \) and \( r > 0 \), though~\cite{kalle} says something different.

\asteriskline
Now the main \emph{characterization} of uniform integrability: \( (X_j)_j \)
are uniformly integrable if and only if
\begin{equation}
	\sup_{j \in J} \E \abs{X_j} < \infty \quad \text{and} \quad \lim_{\P(A) \un} \sup_{j \in J}
	\E(\abs{X_j} \whi A) = 0.
\end{equation}
\emph{Only if.}
Set arbitrary \( \varepsilon > 0 \) and choose \( r_0 \) so that
\( \sup_j \E(\abs{X_j} \whi \abs{X_j} > r) < \varepsilon \) for \( r \ge r_0 \).
\begin{align}
	\E \abs{X_j} & = \E (\abs{X_j} \whi \abs{X_j} \le r_0) + \E(\abs{X_j} \whi \abs{X_j} > r_0) \\
	             & \le r_0 + \E(\abs{X_j} \whi \abs{X_j} > r_0)
	\le r_0 + \varepsilon
\end{align}
This proves the first part.

More generally for any \( r > 0 \),
\begin{align}
	\E(\abs{X_j} \whi A) & = \E(\abs{X_j} \whi \abs{X_j} \le r, A)
	+ \E(\abs{X_j} \whi \abs{X_j} >r, A)                                     \\
	                     & \le r\P(A) + \E(\abs{X_j} \whi \abs{X_j} > r) \un
\end{align}
as \( \P(A) \un \) and \( r \ub \) and uniformly in \( j \). This proves the second part.

\emph{If.} Define \( A_{j,r} = \vit{\abs{X_j} > r} \). Then the probabilities of these events are bounded
uniformly in \( j \) by Markov's inequality:
\begin{equation}
	\sup_j \P(A_{j,r}) \le r^{-1} \sup_j \E \abs{X_j}.
\end{equation}

For any \( \varepsilon > 0 \) and large enough \( r \), the assumption implies
\begin{equation}
	\sup_{j,k \in J } \E(\abs{X_j} \whi A_{k, r}) < \varepsilon,
\end{equation}
which in particular gives the desired uniform integrability by taking only \( k=r \).

\asteriskline
A similar technique can be used to prove the following: if \( X \) is integrable,
then
\begin{equation}
	\lim_{\P(A) \un} \E (\abs X \whi A) = 0.
\end{equation}
We know that
\begin{equation}
	\lim_{r \ub} \E(\abs X \whi \abs X > r) = 0
\end{equation}
because \( X \cdot 1_{\vit{X > r}} \un \) and dominated convergence. The
\enquote{rest of \( A \)} is not a problem since
\begin{equation}
	\E(\abs X \whi A) \le r\P(A) + \E(\abs X \whi \abs X > r).
\end{equation}

\subsubsection{Convergence of means}
For random variables \( X, X_1, X_2, \ldots \) with \( X_n \konvd X \) we have
\begin{equation}\label{eq:unifco-conmean}
	\E X_n \to \E X < \infty \iff  X_n \text{ are uniformly integrable.}
\end{equation}

\emph{\fbox { \( \Leftarrow \) }}  To get \( \E X < \infty \), we switch to \emph{bounded} functions. For all \( r > 0 \),
\begin{equation}
	\liminf_{n \ub} \E X_n \ge \lim_{n \ub} \E \ob{X_n \wedge r} = \E \ob{X \wedge r}
\end{equation}
so with \( r \ub \) we have \( \E X \le \liminf_n \E X_n < \infty  \).

The other part follows from
\begin{equation}
	\abs{\E X_n - \E X}
	\le \abs{\E X_n - \E \ob {X_n \wedge r} }
	+ \abs{\E \ob{X_n \wedge r} - \E \ob{X \wedge r} }
	+ \abs{\E \ob{X \wedge r} - \E X}
\end{equation}
for all \( r > 0 \), wherein we first let \( n \ub \) and then \( r \ub \), the uniformity being key.

\emph{\fbox { \( \Rightarrow \)}}
If \( X \) is \emph{continuous}, then \( \E(X_n \whi X_n > r) \to \E(X \whi X > r) \) by
the continuous mapping theorem.

More generally, we need to replace \( x \mapsto x \cdot 1(x > r) \) with a \( \ge \) continuous function;
\cite{kalle} offers
\begin{equation}
	g(x) = x - \ug{x \wedge (r-x)_+}
\end{equation}
which is more nicely written as
\( g(x) = 0 \) if \( x \le r/2 \); \( 2x-r \) if \( r/2 < x \le r \) and \( x \) otherwise. Then
\begin{equation}
	\E\ob{X_n \whi X_n > r} \le \E{g(X_n)} \stackrel n \rightarrow \E{g(X)} \stackrel r \rightarrow 0
\end{equation}
with, again, obvious uniformity in \( n \).


% \subsection{Stochastic processes and finite-dimensional distributions}

\subsection{Independence and \zerone \ laws}

% \subsection{Constructing random variables with given distributions}

\subsection{Notable exercises}
\subsubsection{Asymptotics of tail-expectations}
See~\cite[ex.\ 1.6.14]{du}. Let \( X \ge 0 \) (without assuming \( E(1/X) < \infty \)). Then
\begin{equation}
	\lim_{y \ub} y\E \ob{\frac 1X \whi X > y} = 0, \quad
	\lim_{y \downarrow 0} y\E \ob{\frac 1X \whi X > y} = 0.
\end{equation}

For the \emph{first} part,
\begin{equation}
	\E \ob{\frac yX \whi X > y} \le \E (1\whi X > y) = \P(X > y) \un.
\end{equation}

For the \emph{second} part,
denoting \( n = 1/y \) and replacing \( X \) with \( 1/X \), the limit becomes
\( \lim_{n \ub} \frac 1n \E(X; X < n) \) and we have
\begin{equation}
	\frac 1n \E(X \whi X \le n) = \frac 1n \int_0^n x \D F
	= \frac 1n \int_0^{\sqrt n} x \D F + \frac 1n \int_{\sqrt n} ^n x\D F \un,
\end{equation}
because
\begin{equation}
	\int_0^{\sqrt n} x \D F \le \sqrt n, \qquad
	\int_{\sqrt n}^n x \D F \le n \ug{F(n) - F(\sqrt n)}.
\end{equation}

\subsubsection{For \( p < q \), \( \L^q \)-convergence implies \( \L^p \)-convergence}
See~\cite[ex.\ 1.6.11]{du}. Let \( 0 < p < q \) and \( \E \abs X ^q < \infty \). Then
\begin{equation}
	\E \abs X^p \le \ob{\E \abs X^q}^{p/q}.
\end{equation}
In particular,
\begin{itemize}
	\item the \( p \)-norm is \emph{smaller}; by taking the root we get \( \norm X_p \le \norm X_q \),
	\item the \( p \)-norm is \emph{finite}, so if \( X \) is \( q \)-integrable it is \( p \)-integrable,
	\item \emph{convergence} in \( \L^q \) implies convergence in \( \L^p \).
\end{itemize}

To \emph{prove} it, note that
\begin{equation}
	\E \abs X^p = \E \ug{\ob{\abs X^q}^{p/q}}
	\le \ob{\E \abs X^q}^{p/q}
\end{equation}
where the inequality is Jensen for concave \( x \mapsto x^{p/q} \).

\subsubsection{Integrating with respect to Lebesgue--Stieltjes measures}
For \( F, G \) corresponding to \( \mu, \nu \) measures on \( (\R, \mathcal B) \),
\begin{enumerate}[label=(\alph*)]
	\item \( \int_{\oc{a,b}} \ug{F(y)-F(a)} \ime Gy =
	      (\mu \otimes \nu) \vit{(x, y) \st a < x \le y < b} \),
	\item
	      \( \displaystyle \int_{\oc{a,b}} F(y) \ime Gy + \int_{\oc{a,b}} G(y) \ime Fy
	      = F(b)G(b) - F(a)G(a) + \sum_{x \in \oc{a,b} } \mu(\{x\})\nu(\{x\}). \)
	\item In particular, if \( F=G \) is continuous,
	      \begin{equation}
		      \int_{\oc{a,b}} 2F(y) \ime Fy = F(b)^2-F(a)^2.
	      \end{equation}
\end{enumerate}

\emph{Firstly}, denote \( D_{a,b} = \vit{(x,y) \st a < x \le y \le b} \).
Then,
\begin{align}
	(\mu \otimes \nu)(D_{a,b}) & = \int \mu(D_{a,b}^y) \ime \nu y           \\
	                           & = \int_{\oc{a,b}} \mu(\oc{a,y}) \ime \nu y \\ &=
	\int_{\oc{a,b}} \ug{F(b)-F(a)} \ime Gy.
\end{align}

\emph{Secondly},
\begin{align}
	\int_{\oc{a,b}} F(y) \ime Gy & = \int_{\oc{a,b}} \ug{F(y)-F(a) + F(a)} \ime Gy                \\
	                             & = \int_{\oc{a,b}} \ug{F(y)-F(a)} \ime Gy + F(a) \ug{G(b)-G(a)} \\ &=
	(\mu \otimes \nu)(D_{a,b}) + F(a)\ug{G(b)-G(a)}.
\end{align}
Analogously,
\begin{align}
	\int_{\oc{a,b}} G(y) \ime Fy = (\mu \otimes \nu)(D_{a,b}') + G(a) \ug{F(b)-F(a)}
\end{align}
where \( D_{a,b}' = \vit{(x,y) \st a < y \le x \le b} \).
The final result follows from
\begin{align}
	( \mu \otimes \nu)(D_{a,b}) + (\mu \otimes \nu)(D_{a,b}')
	 & = (\mu \otimes \nu)(D_{a,b} \cup D_{a,b}') + (\mu \otimes \nu)(D_{a,b} \cap D_{a,b}') \\
	 & = \ug{F(b)-F(a)}\ug{G(b)-G(a)} + \sum_{x \in \oc{a,b}} \mu(\{x\}) \nu(\{ x\}).
\end{align}

